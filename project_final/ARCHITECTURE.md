# Architecture Overview  
**ClickHouse Real-Time Analytics Platform (Batch vs Streaming)**

# Проектная работа

## Платформа аналитики реального времени для высокочастотных рыночных данных  
## на базе ClickHouse: batch и streaming ingestion
---

## 1. Цель архитектуры

Цель проекта — продемонстрировать возможности ClickHouse по обработке и аналитике
высокочастотных событий в режиме, близком к real-time, с одновременным использованием:

- batch ingestion (периодическая загрузка данных),
- streaming ingestion (потоковая обработка),
- многоуровневой архитектуры хранения,
- управляемого жизненного цикла данных (data lifecycle).

В качестве основного источника используются реальные рыночные события
(криптовалютные сделки), дополненные внешними макро-факторами
(данные о потреблении электроэнергии).

---

## 2. Источники данных

### 2.1 Криптовалютные сделки (High-Frequency Events)

**Источник:**  
Binance Spot API (`aggTrades`)

**Торговые пары:**
- `BTCUSDT`
- `ETHUSDT`
- `BTCUSDC`
- `ETHUSDC`

**Характеристики потока (эмпирически измерено):**
- ~950 000 событий в сутки (все пары суммарно),
- средняя плотность: 5–10 событий в секунду,
- пиковая плотность: >1000 событий в секунду,
- неравномерный, bursty-трафик.

**Способы поступления данных:**
- batch-пайплайн (Apache Airflow),
- streaming-пайплайн (Kafka).

---

### 2.2 Энергопотребление (Low-Frequency External Factor)

**Источник:**  
U.S. Energy Information Administration (EIA API)

**Характеристики:**
- почасовая гранулярность,
- существенно меньший объём данных,
- используется как внешний аналитический фактор
  для корреляции с рыночной активностью.

---

## 3. Архитектурные слои хранения данных

Архитектура реализует **многоуровневый подход**, разделяющий ingestion,
аналитику и долговременное хранение данных.

---

### 3.1 Raw Ingestion Layer (HOT)

**Назначение:**
- приём batch и streaming данных,
- хранение максимальной детализации (сырые сделки),
- отсутствие прямых аналитических запросов.

**Реализация:**
- `ReplicatedMergeTree`,
- партиционирование по дате: `toDate(event_time)`,
- сортировка: `(symbol, event_time, trade_id)`.

**Хранение:**
- временное (HOT layer),
- ограниченный срок жизни.

**Очистка данных:**
- `DROP PARTITION` после архивации **или**
- TTL (3–7 дней).

Данный слой используется **исключительно как источник**
для downstream-агрегаций.

---

### 3.2 Aggregation Layer (WARM)

**Назначение:**
- формирование агрегатов из raw-событий,
- подготовка данных для аналитики и BI.

**Реализация:**
- Materialized Views в ClickHouse.

**Поддерживаемые агрегации:**
- 1 секунда
- 1 минута
- 15 минут
- 1 час
- 1 день

Данные в этом слое:
- агрегированы по времени,
- значительно меньше по объёму,
- используются для аналитических витрин.

---

### 3.3 Data Mart Layer (Analytics / BI)

**Назначение:**
- прямое подключение BI-инструментов,
- быстрые фильтры и JOIN-операции,
- минимальная нагрузка на кластер.

**Инструмент визуализации:**
- Yandex DataLens.

**Особенности:**
- только агрегированные данные,
- JOIN с данными энергопотребления,
- витрины только на чтение.

Этот слой является **основным потребителем данных** в проекте.

---

### 3.4 Cold Storage Layer (ARCHIVE)

**Назначение:**
- долговременное хранение сырых данных,
- возможность повторной переработки,
- снижение нагрузки на ClickHouse.

**Реализация:**
- Object Storage (S3 / MinIO),
- формат хранения: Parquet.

**Процесс:**
1. Ежедневный экспорт партиций raw-данных.
2. Проверка целостности (row count, контрольные суммы).
3. Удаление партиций из ClickHouse.

---

## 4. Пайплайны обработки данных

### 4.1 Batch Pipeline (Apache Airflow)

Используется для:
- загрузки исторических данных,
- контролируемой периодической обработки,
- демонстрации идемпотентных операций.

**Особенности:**
- загрузка временными окнами (например, 5 минут),
- staging-таблицы,
- атомарные операции (`REPLACE PARTITION`).

---

### 4.2 Streaming Pipeline (Kafka)

Используется для:
- near real-time обработки событий,
- демонстрации потоковой архитектуры.

**Компоненты:**
- Kafka Topics,
- ClickHouse Kafka Engine,
- Materialized Views для записи в raw-таблицы.

Пайплайн рассчитан на bursty-нагрузку
и пики >1000 сообщений в секунду.

---

## 5. Управление жизненным циклом данных

В проекте реализован полный lifecycle данных:

1. **Ingest** — batch и stream.
2. **Aggregate** — Materialized Views.
3. **Analyze** — Data Marts / BI.
4. **Archive** — Object Storage.
5. **Cleanup** — TTL / DROP PARTITION.

Это позволяет:
- контролировать рост хранилища,
- разделять ingestion и аналитические нагрузки,
- приближаться к production-архитектурам.

---

## 6. Кластер ClickHouse

**Конфигурация:**
- 1 shard,
- 2 replicas,
- ClickHouse Keeper (3 узла).

Кластер обеспечивает:
- отказоустойчивость,
- репликацию данных,
- корректную работу Distributed-таблиц.

---

## 7. Резервное копирование и восстановление (Backup & Recovery)

В архитектуре предусмотрена многоуровневая система резервного копирования,
обеспечивающая защиту данных от логических и физических повреждений,
а также возможность полного восстановления кластера.

**Основные компоненты:**
- `clickhouse-backup`,
- S3-совместимое хранилище (MinIO).

Поддерживаются:
- полные бэкапы,
- дифференциальные бэкапы,
- инкрементальные бэкапы.

Резервное копирование интегрировано с жизненным циклом данных
и cold storage-слоем.

---

## 8. Ключевые архитектурные принципы

- Raw-данные не используются напрямую для аналитики.
- Все BI-запросы работают только с агрегированными витринами.
- Высокочастотные данные имеют ограниченный срок жизни.
- Агрегаты хранятся долгосрочно.
- Внешние факторы интегрируются через JOIN по времени.

---

## 9. Итог

Архитектура демонстрирует:
- работу ClickHouse с high-frequency данными,
- различия batch и streaming ingestion,
- использование Materialized Views,
- управление партициями и TTL,
- интеграцию с BI и внешними факторами,
- production-подходы к резервному копированию и хранению.

Проект ориентирован на практическое применение
и максимально приближен к реальным сценариям эксплуатации
аналитических платформ.